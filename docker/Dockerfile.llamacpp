FROM dustynv/l4t-pytorch:r36.2.0

# Set working directory
WORKDIR /root/llamacpp

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    cmake \
    build-essential \
    libcurl4-openssl-dev \
    libssl-dev \
    libzstd-dev \
    libfmt-dev \
    libyaml-cpp-dev \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Copy the GGUF model into the container
COPY gpt-oss-20b-Q4_K_M.gguf ./

# Clone llama.cpp with submodules (needed for Jinja/inja)
RUN git clone --recurse-submodules https://github.com/ggml-org/llama.cpp.git \
    && cd llama.cpp \
    && cmake -B build -DGGML_CUDA=ON -DPYTHON_EXECUTABLE=$(which python3) \
    && cmake --build build --parallel

# # Optional: set PATH to find llama-server easily
# ENV PATH="/root/llamacpp/llama.cpp/build/bin:${PATH}"

# # Default working directory
# WORKDIR /root/llamacpp

# # Default entrypoint
# CMD ["bash"]
